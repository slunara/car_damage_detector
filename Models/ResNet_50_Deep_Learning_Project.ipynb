{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Setup Environment & Load Libraries***"
      ],
      "metadata": {
        "id": "lIzZFeLcUNIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6Jk67XTSr68",
        "outputId": "dcaea38a-bef5-4fbd-e625-953d469bed12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install torch torchvision numpy matplotlib scikit-learn tqdm\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Load & Preprocess Dataset***"
      ],
      "metadata": {
        "id": "O4sA0Uv9UW3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"/content/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEyXkZSnboVx",
        "outputId": "29c46e01-ccdb-4c1e-bb96-c984b6d2e640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'TQVCD-main.zip', 'TQVCD', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/TQVCD-main.zip\"  # Ensure this matches the actual filename\n",
        "extract_path = \"/content/TQVCD\"\n",
        "\n",
        "# Extract ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"‚úÖ Dataset extracted successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76cX5u9ycpJ6",
        "outputId": "f99f0c2b-9ab7-4f37-e606-2e514bed636b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_images = sum([len(files) for _, _, files in os.walk(\"/content/TQVCD\")])\n",
        "print(f\"‚úÖ Total images in dataset: {total_images}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEEgpWTDco53",
        "outputId": "e7c66140-0bfc-4fff-a444-4982d036aece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Total images in dataset: 268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Organize Dataset (Binary Classification)***"
      ],
      "metadata": {
        "id": "n_WZwuWLiKTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create binary classification folders\n",
        "os.makedirs(\"/content/TQVCD_Binary/normal\", exist_ok=True)\n",
        "os.makedirs(\"/content/TQVCD_Binary/damaged\", exist_ok=True)\n",
        "\n",
        "# Define dataset folder mappings\n",
        "source_folders = {\n",
        "    \"normal\": [\"FN\", \"RN\"],  # Undamaged cars\n",
        "    \"damaged\": [\"FB\", \"FC\", \"RB\", \"RC\"]  # Damaged cars\n",
        "}\n",
        "\n",
        "# Move images into binary folders\n",
        "for label, folders in source_folders.items():\n",
        "    for folder in folders:\n",
        "        folder_path = os.path.join(\"/content/TQVCD\", folder)\n",
        "        if os.path.exists(folder_path):\n",
        "            for img_name in os.listdir(folder_path):\n",
        "                img_src = os.path.join(folder_path, img_name)\n",
        "                img_dst = os.path.join(f\"/content/TQVCD_Binary/{label}\", img_name)\n",
        "                shutil.move(img_src, img_dst)\n",
        "\n",
        "print(\"‚úÖ Dataset reorganized into binary classification format!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naPHJkxnVqTk",
        "outputId": "32454d64-2d93-47f4-91cc-1537f369d92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset reorganized into binary classification format!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Load and Preprocess Dataset***"
      ],
      "metadata": {
        "id": "yX7TowtpiR0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Define dataset path (Binary classification)\n",
        "DATA_DIR_BINARY = \"/content/TQVCD_Binary\"\n",
        "\n",
        "# Data augmentation for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.RandomAffine(15, translate=(0.2, 0.2), shear=10),  # Stronger transformations\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Simple transform for validation and testing\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset = datasets.ImageFolder(DATA_DIR_BINARY, transform=transform_train)\n",
        "\n",
        "# Split dataset (60% Train, 20% Validation, 20% Test)\n",
        "train_size = int(0.6 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Apply different transformations for validation and test sets\n",
        "val_dataset.dataset.transform = transform_test\n",
        "test_dataset.dataset.transform = transform_test\n",
        "\n",
        "# Define DataLoaders\n",
        "BATCH_SIZE = 16  # Small dataset ‚Üí Small batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Print dataset split\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TNKrjVfWVHH",
        "outputId": "05c8489c-e9a2-47c3-98af-20a9fd911535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded successfully!\n",
            "Train samples: 72, Validation samples: 24, Test samples: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Define ResNet-50 Model (Transfer Learning)***"
      ],
      "metadata": {
        "id": "RhISBsBEieyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load pre-trained ResNet-50\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze all layers except the final layer\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify final layer for binary classification\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 1)  # Single output neuron for binary classification\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "\n",
        "print(\"‚úÖ ResNet-50 Model Loaded & Ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fwtBL9nihBe",
        "outputId": "26d8128b-4566-45da-b132-c9faeffd90ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 123MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ResNet-50 Model Loaded & Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement Grid Search for Hyperparameter Tuning***"
      ],
      "metadata": {
        "id": "0FfW8UNBik6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "import time\n",
        "\n",
        "# Define reduced hyperparameter grid\n",
        "LEARNING_RATES = [1e-3, 1e-4]  # Focus on stable training\n",
        "BATCH_SIZES = [8, 16]  # Small dataset ‚Üí Small batch sizes\n",
        "EPOCHS_LIST = [20, 30]  # Testing different training durations\n",
        "OPTIMIZERS = [\"adam\", \"sgd\"]  # Adam adapts, SGD is robust\n",
        "\n",
        "best_model_params = None\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# Iterate over all hyperparameter combinations\n",
        "for lr, bs, ep, opt in product(LEARNING_RATES, BATCH_SIZES, EPOCHS_LIST, OPTIMIZERS):\n",
        "    print(f\"\\nüîç Testing: LR={lr}, BS={bs}, Epochs={ep}, Optimizer={opt}\")\n",
        "\n",
        "    # Define DataLoaders with current batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
        "\n",
        "    # Load pre-trained ResNet-50 model\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, 1)  # Binary classification\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define Loss Function\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Define Optimizer\n",
        "    if opt == \"adam\":\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    elif opt == \"sgd\":\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    # Training Loop with Early Stopping\n",
        "    patience = 5\n",
        "    best_loss = float(\"inf\")\n",
        "    early_stop_count = 0\n",
        "\n",
        "    for epoch in range(ep):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images).squeeze(1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n",
        "                outputs = model(images).squeeze(1)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{ep}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Early Stopping\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            early_stop_count = 0\n",
        "        else:\n",
        "            early_stop_count += 1\n",
        "            if early_stop_count >= patience:\n",
        "                print(\"‚èπ Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # Track Best Hyperparameter Set\n",
        "    if best_loss < best_val_loss:\n",
        "        best_val_loss = best_loss\n",
        "        best_model_params = (lr, bs, ep, opt)\n",
        "\n",
        "print(f\"\\nüî• Best Hyperparameters: LR={best_model_params[0]}, BS={best_model_params[1]}, Epochs={best_model_params[2]}, Optimizer={best_model_params[3]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEH04q2gikJ3",
        "outputId": "c75ed5e6-849f-445b-bc33-97f494e56fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Testing: LR=0.001, BS=8, Epochs=20, Optimizer=adam\n",
            "Epoch [1/20] - Train Loss: 0.8692, Val Loss: 31.8546\n",
            "Epoch [2/20] - Train Loss: 0.7328, Val Loss: 1.1490\n",
            "Epoch [3/20] - Train Loss: 0.4738, Val Loss: 1.9574\n",
            "Epoch [4/20] - Train Loss: 0.3306, Val Loss: 0.7494\n",
            "Epoch [5/20] - Train Loss: 0.3155, Val Loss: 1.4959\n",
            "Epoch [6/20] - Train Loss: 0.3561, Val Loss: 2.2659\n",
            "Epoch [7/20] - Train Loss: 0.4033, Val Loss: 0.8900\n",
            "Epoch [8/20] - Train Loss: 0.2269, Val Loss: 0.9737\n",
            "Epoch [9/20] - Train Loss: 0.2490, Val Loss: 0.7823\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.001, BS=8, Epochs=20, Optimizer=sgd\n",
            "Epoch [1/20] - Train Loss: 0.6989, Val Loss: 0.8045\n",
            "Epoch [2/20] - Train Loss: 0.5840, Val Loss: 0.6765\n",
            "Epoch [3/20] - Train Loss: 0.5186, Val Loss: 0.6817\n",
            "Epoch [4/20] - Train Loss: 0.4436, Val Loss: 0.6225\n",
            "Epoch [5/20] - Train Loss: 0.3424, Val Loss: 0.6321\n",
            "Epoch [6/20] - Train Loss: 0.2459, Val Loss: 0.5914\n",
            "Epoch [7/20] - Train Loss: 0.1663, Val Loss: 0.6645\n",
            "Epoch [8/20] - Train Loss: 0.1054, Val Loss: 0.6108\n",
            "Epoch [9/20] - Train Loss: 0.0825, Val Loss: 0.6088\n",
            "Epoch [10/20] - Train Loss: 0.0456, Val Loss: 0.6229\n",
            "Epoch [11/20] - Train Loss: 0.0394, Val Loss: 0.6218\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.001, BS=8, Epochs=30, Optimizer=adam\n",
            "Epoch [1/30] - Train Loss: 1.0250, Val Loss: 21.0790\n",
            "Epoch [2/30] - Train Loss: 0.6006, Val Loss: 1.7860\n",
            "Epoch [3/30] - Train Loss: 0.3190, Val Loss: 5.1139\n",
            "Epoch [4/30] - Train Loss: 0.3558, Val Loss: 2.5257\n",
            "Epoch [5/30] - Train Loss: 0.1621, Val Loss: 1.6993\n",
            "Epoch [6/30] - Train Loss: 0.1559, Val Loss: 1.5183\n",
            "Epoch [7/30] - Train Loss: 0.3967, Val Loss: 1.4249\n",
            "Epoch [8/30] - Train Loss: 0.2408, Val Loss: 0.7339\n",
            "Epoch [9/30] - Train Loss: 0.1727, Val Loss: 0.8194\n",
            "Epoch [10/30] - Train Loss: 0.0854, Val Loss: 1.7694\n",
            "Epoch [11/30] - Train Loss: 0.2619, Val Loss: 1.7039\n",
            "Epoch [12/30] - Train Loss: 0.2799, Val Loss: 8.3289\n",
            "Epoch [13/30] - Train Loss: 0.5899, Val Loss: 1.1667\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.001, BS=8, Epochs=30, Optimizer=sgd\n",
            "Epoch [1/30] - Train Loss: 0.6558, Val Loss: 0.7593\n",
            "Epoch [2/30] - Train Loss: 0.5686, Val Loss: 0.6564\n",
            "Epoch [3/30] - Train Loss: 0.4985, Val Loss: 0.6771\n",
            "Epoch [4/30] - Train Loss: 0.4069, Val Loss: 0.6483\n",
            "Epoch [5/30] - Train Loss: 0.3515, Val Loss: 0.6409\n",
            "Epoch [6/30] - Train Loss: 0.2899, Val Loss: 0.7321\n",
            "Epoch [7/30] - Train Loss: 0.1669, Val Loss: 0.5844\n",
            "Epoch [8/30] - Train Loss: 0.1160, Val Loss: 0.6591\n",
            "Epoch [9/30] - Train Loss: 0.0634, Val Loss: 0.6772\n",
            "Epoch [10/30] - Train Loss: 0.0714, Val Loss: 0.6315\n",
            "Epoch [11/30] - Train Loss: 0.0334, Val Loss: 0.6481\n",
            "Epoch [12/30] - Train Loss: 0.0224, Val Loss: 0.6450\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.001, BS=16, Epochs=20, Optimizer=adam\n",
            "Epoch [1/20] - Train Loss: 0.9078, Val Loss: 3.0328\n",
            "Epoch [2/20] - Train Loss: 0.5274, Val Loss: 5.5708\n",
            "Epoch [3/20] - Train Loss: 0.2204, Val Loss: 1.6326\n",
            "Epoch [4/20] - Train Loss: 0.3514, Val Loss: 15.7222\n",
            "Epoch [5/20] - Train Loss: 0.2372, Val Loss: 35.3596\n",
            "Epoch [6/20] - Train Loss: 0.5081, Val Loss: 9.5058\n",
            "Epoch [7/20] - Train Loss: 0.3208, Val Loss: 4.9245\n",
            "Epoch [8/20] - Train Loss: 0.2260, Val Loss: 2.0450\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.001, BS=16, Epochs=20, Optimizer=sgd\n",
            "Epoch [1/20] - Train Loss: 0.6739, Val Loss: 0.6376\n",
            "Epoch [2/20] - Train Loss: 0.5878, Val Loss: 0.6321\n",
            "Epoch [3/20] - Train Loss: 0.5528, Val Loss: 0.6049\n",
            "Epoch [4/20] - Train Loss: 0.4970, Val Loss: 0.6003\n",
            "Epoch [5/20] - Train Loss: 0.4211, Val Loss: 0.5820\n",
            "Epoch [6/20] - Train Loss: 0.3700, Val Loss: 0.5932\n",
            "Epoch [7/20] - Train Loss: 0.3009, Val Loss: 0.5669\n",
            "Epoch [8/20] - Train Loss: 0.2408, Val Loss: 0.5565\n",
            "Epoch [9/20] - Train Loss: 0.2359, Val Loss: 0.5555\n",
            "Epoch [10/20] - Train Loss: 0.1930, Val Loss: 0.5400\n",
            "Epoch [11/20] - Train Loss: 0.1259, Val Loss: 0.5498\n",
            "Epoch [12/20] - Train Loss: 0.1214, Val Loss: 0.5552\n",
            "Epoch [13/20] - Train Loss: 0.0738, Val Loss: 0.5714\n",
            "Epoch [14/20] - Train Loss: 0.0616, Val Loss: 0.5702\n",
            "Epoch [15/20] - Train Loss: 0.0803, Val Loss: 0.5694\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.001, BS=16, Epochs=30, Optimizer=adam\n",
            "Epoch [1/30] - Train Loss: 0.8743, Val Loss: 10.4178\n",
            "Epoch [2/30] - Train Loss: 0.4650, Val Loss: 4.9846\n",
            "Epoch [3/30] - Train Loss: 0.3339, Val Loss: 21.7702\n",
            "Epoch [4/30] - Train Loss: 0.1757, Val Loss: 67.3282\n",
            "Epoch [5/30] - Train Loss: 0.1184, Val Loss: 14.0987\n",
            "Epoch [6/30] - Train Loss: 0.1639, Val Loss: 3.7529\n",
            "Epoch [7/30] - Train Loss: 0.1520, Val Loss: 3.9533\n",
            "Epoch [8/30] - Train Loss: 0.2379, Val Loss: 0.8163\n",
            "Epoch [9/30] - Train Loss: 0.2055, Val Loss: 2.3461\n",
            "Epoch [10/30] - Train Loss: 0.1178, Val Loss: 0.9509\n",
            "Epoch [11/30] - Train Loss: 0.0947, Val Loss: 0.9042\n",
            "Epoch [12/30] - Train Loss: 0.0414, Val Loss: 0.6508\n",
            "Epoch [13/30] - Train Loss: 0.0503, Val Loss: 0.9333\n",
            "Epoch [14/30] - Train Loss: 0.0162, Val Loss: 1.1876\n",
            "Epoch [15/30] - Train Loss: 0.0593, Val Loss: 1.1378\n",
            "Epoch [16/30] - Train Loss: 0.0137, Val Loss: 1.1029\n",
            "Epoch [17/30] - Train Loss: 0.0301, Val Loss: 0.8378\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.001, BS=16, Epochs=30, Optimizer=sgd\n",
            "Epoch [1/30] - Train Loss: 0.6692, Val Loss: 0.6469\n",
            "Epoch [2/30] - Train Loss: 0.6179, Val Loss: 0.6388\n",
            "Epoch [3/30] - Train Loss: 0.5585, Val Loss: 0.6230\n",
            "Epoch [4/30] - Train Loss: 0.5202, Val Loss: 0.6072\n",
            "Epoch [5/30] - Train Loss: 0.4259, Val Loss: 0.5992\n",
            "Epoch [6/30] - Train Loss: 0.3821, Val Loss: 0.5874\n",
            "Epoch [7/30] - Train Loss: 0.3252, Val Loss: 0.5807\n",
            "Epoch [8/30] - Train Loss: 0.2491, Val Loss: 0.5779\n",
            "Epoch [9/30] - Train Loss: 0.2057, Val Loss: 0.5720\n",
            "Epoch [10/30] - Train Loss: 0.1513, Val Loss: 0.5645\n",
            "Epoch [11/30] - Train Loss: 0.1275, Val Loss: 0.5586\n",
            "Epoch [12/30] - Train Loss: 0.1222, Val Loss: 0.5676\n",
            "Epoch [13/30] - Train Loss: 0.0767, Val Loss: 0.5600\n",
            "Epoch [14/30] - Train Loss: 0.0655, Val Loss: 0.5533\n",
            "Epoch [15/30] - Train Loss: 0.0523, Val Loss: 0.5496\n",
            "Epoch [16/30] - Train Loss: 0.0603, Val Loss: 0.5631\n",
            "Epoch [17/30] - Train Loss: 0.0452, Val Loss: 0.5484\n",
            "Epoch [18/30] - Train Loss: 0.0362, Val Loss: 0.5505\n",
            "Epoch [19/30] - Train Loss: 0.0271, Val Loss: 0.5479\n",
            "Epoch [20/30] - Train Loss: 0.0314, Val Loss: 0.5490\n",
            "Epoch [21/30] - Train Loss: 0.0466, Val Loss: 0.5628\n",
            "Epoch [22/30] - Train Loss: 0.0244, Val Loss: 0.5586\n",
            "Epoch [23/30] - Train Loss: 0.0259, Val Loss: 0.5548\n",
            "Epoch [24/30] - Train Loss: 0.0155, Val Loss: 0.5555\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.0001, BS=8, Epochs=20, Optimizer=adam\n",
            "Epoch [1/20] - Train Loss: 0.6569, Val Loss: 0.6287\n",
            "Epoch [2/20] - Train Loss: 0.2708, Val Loss: 0.5368\n",
            "Epoch [3/20] - Train Loss: 0.1077, Val Loss: 0.5988\n",
            "Epoch [4/20] - Train Loss: 0.1009, Val Loss: 0.4795\n",
            "Epoch [5/20] - Train Loss: 0.0934, Val Loss: 0.6152\n",
            "Epoch [6/20] - Train Loss: 0.1296, Val Loss: 0.5051\n",
            "Epoch [7/20] - Train Loss: 0.0816, Val Loss: 0.5128\n",
            "Epoch [8/20] - Train Loss: 0.0862, Val Loss: 0.5530\n",
            "Epoch [9/20] - Train Loss: 0.1210, Val Loss: 0.6940\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.0001, BS=8, Epochs=20, Optimizer=sgd\n",
            "Epoch [1/20] - Train Loss: 0.7034, Val Loss: 0.6794\n",
            "Epoch [2/20] - Train Loss: 0.6439, Val Loss: 0.6867\n",
            "Epoch [3/20] - Train Loss: 0.6171, Val Loss: 0.6879\n",
            "Epoch [4/20] - Train Loss: 0.5964, Val Loss: 0.6868\n",
            "Epoch [5/20] - Train Loss: 0.6029, Val Loss: 0.6909\n",
            "Epoch [6/20] - Train Loss: 0.5852, Val Loss: 0.6741\n",
            "Epoch [7/20] - Train Loss: 0.5636, Val Loss: 0.6794\n",
            "Epoch [8/20] - Train Loss: 0.5484, Val Loss: 0.6741\n",
            "Epoch [9/20] - Train Loss: 0.5438, Val Loss: 0.6730\n",
            "Epoch [10/20] - Train Loss: 0.5351, Val Loss: 0.6680\n",
            "Epoch [11/20] - Train Loss: 0.5197, Val Loss: 0.6607\n",
            "Epoch [12/20] - Train Loss: 0.5006, Val Loss: 0.6618\n",
            "Epoch [13/20] - Train Loss: 0.5135, Val Loss: 0.6624\n",
            "Epoch [14/20] - Train Loss: 0.4900, Val Loss: 0.6609\n",
            "Epoch [15/20] - Train Loss: 0.4778, Val Loss: 0.6601\n",
            "Epoch [16/20] - Train Loss: 0.4577, Val Loss: 0.6578\n",
            "Epoch [17/20] - Train Loss: 0.4413, Val Loss: 0.6515\n",
            "Epoch [18/20] - Train Loss: 0.4449, Val Loss: 0.6524\n",
            "Epoch [19/20] - Train Loss: 0.4450, Val Loss: 0.6438\n",
            "Epoch [20/20] - Train Loss: 0.4079, Val Loss: 0.6439\n",
            "\n",
            "üîç Testing: LR=0.0001, BS=8, Epochs=30, Optimizer=adam\n",
            "Epoch [1/30] - Train Loss: 0.6211, Val Loss: 0.6230\n",
            "Epoch [2/30] - Train Loss: 0.2675, Val Loss: 0.6670\n",
            "Epoch [3/30] - Train Loss: 0.1195, Val Loss: 0.6443\n",
            "Epoch [4/30] - Train Loss: 0.0545, Val Loss: 0.4960\n",
            "Epoch [5/30] - Train Loss: 0.0640, Val Loss: 0.6425\n",
            "Epoch [6/30] - Train Loss: 0.1092, Val Loss: 0.7943\n",
            "Epoch [7/30] - Train Loss: 0.0319, Val Loss: 0.9453\n",
            "Epoch [8/30] - Train Loss: 0.0141, Val Loss: 0.8530\n",
            "Epoch [9/30] - Train Loss: 0.0358, Val Loss: 0.9649\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.0001, BS=8, Epochs=30, Optimizer=sgd\n",
            "Epoch [1/30] - Train Loss: 0.6517, Val Loss: 0.6794\n",
            "Epoch [2/30] - Train Loss: 0.6273, Val Loss: 0.6839\n",
            "Epoch [3/30] - Train Loss: 0.6137, Val Loss: 0.6860\n",
            "Epoch [4/30] - Train Loss: 0.6001, Val Loss: 0.6854\n",
            "Epoch [5/30] - Train Loss: 0.5862, Val Loss: 0.6760\n",
            "Epoch [6/30] - Train Loss: 0.5813, Val Loss: 0.6658\n",
            "Epoch [7/30] - Train Loss: 0.5703, Val Loss: 0.6691\n",
            "Epoch [8/30] - Train Loss: 0.5528, Val Loss: 0.6694\n",
            "Epoch [9/30] - Train Loss: 0.5494, Val Loss: 0.6633\n",
            "Epoch [10/30] - Train Loss: 0.5333, Val Loss: 0.6653\n",
            "Epoch [11/30] - Train Loss: 0.5219, Val Loss: 0.6615\n",
            "Epoch [12/30] - Train Loss: 0.5054, Val Loss: 0.6578\n",
            "Epoch [13/30] - Train Loss: 0.4925, Val Loss: 0.6567\n",
            "Epoch [14/30] - Train Loss: 0.4889, Val Loss: 0.6571\n",
            "Epoch [15/30] - Train Loss: 0.4708, Val Loss: 0.6500\n",
            "Epoch [16/30] - Train Loss: 0.4612, Val Loss: 0.6502\n",
            "Epoch [17/30] - Train Loss: 0.4421, Val Loss: 0.6452\n",
            "Epoch [18/30] - Train Loss: 0.4376, Val Loss: 0.6470\n",
            "Epoch [19/30] - Train Loss: 0.4149, Val Loss: 0.6433\n",
            "Epoch [20/30] - Train Loss: 0.4105, Val Loss: 0.6443\n",
            "Epoch [21/30] - Train Loss: 0.4166, Val Loss: 0.6310\n",
            "Epoch [22/30] - Train Loss: 0.3913, Val Loss: 0.6291\n",
            "Epoch [23/30] - Train Loss: 0.3825, Val Loss: 0.6296\n",
            "Epoch [24/30] - Train Loss: 0.3951, Val Loss: 0.6319\n",
            "Epoch [25/30] - Train Loss: 0.3508, Val Loss: 0.6367\n",
            "Epoch [26/30] - Train Loss: 0.3430, Val Loss: 0.6284\n",
            "Epoch [27/30] - Train Loss: 0.3435, Val Loss: 0.6296\n",
            "Epoch [28/30] - Train Loss: 0.3523, Val Loss: 0.6353\n",
            "Epoch [29/30] - Train Loss: 0.3438, Val Loss: 0.6146\n",
            "Epoch [30/30] - Train Loss: 0.3281, Val Loss: 0.6215\n",
            "\n",
            "üîç Testing: LR=0.0001, BS=16, Epochs=20, Optimizer=adam\n",
            "Epoch [1/20] - Train Loss: 0.5992, Val Loss: 0.5676\n",
            "Epoch [2/20] - Train Loss: 0.2547, Val Loss: 0.6055\n",
            "Epoch [3/20] - Train Loss: 0.0946, Val Loss: 0.7160\n",
            "Epoch [4/20] - Train Loss: 0.0335, Val Loss: 0.7624\n",
            "Epoch [5/20] - Train Loss: 0.0226, Val Loss: 0.6961\n",
            "Epoch [6/20] - Train Loss: 0.0102, Val Loss: 0.7074\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.0001, BS=16, Epochs=20, Optimizer=sgd\n",
            "Epoch [1/20] - Train Loss: 0.6925, Val Loss: 0.6899\n",
            "Epoch [2/20] - Train Loss: 0.6626, Val Loss: 0.6740\n",
            "Epoch [3/20] - Train Loss: 0.6258, Val Loss: 0.6629\n",
            "Epoch [4/20] - Train Loss: 0.6146, Val Loss: 0.6544\n",
            "Epoch [5/20] - Train Loss: 0.6245, Val Loss: 0.6510\n",
            "Epoch [6/20] - Train Loss: 0.6134, Val Loss: 0.6470\n",
            "Epoch [7/20] - Train Loss: 0.5777, Val Loss: 0.6419\n",
            "Epoch [8/20] - Train Loss: 0.6271, Val Loss: 0.6395\n",
            "Epoch [9/20] - Train Loss: 0.5635, Val Loss: 0.6362\n",
            "Epoch [10/20] - Train Loss: 0.5624, Val Loss: 0.6341\n",
            "Epoch [11/20] - Train Loss: 0.5516, Val Loss: 0.6321\n",
            "Epoch [12/20] - Train Loss: 0.5511, Val Loss: 0.6309\n",
            "Epoch [13/20] - Train Loss: 0.5475, Val Loss: 0.6298\n",
            "Epoch [14/20] - Train Loss: 0.5559, Val Loss: 0.6281\n",
            "Epoch [15/20] - Train Loss: 0.5203, Val Loss: 0.6272\n",
            "Epoch [16/20] - Train Loss: 0.5187, Val Loss: 0.6250\n",
            "Epoch [17/20] - Train Loss: 0.4997, Val Loss: 0.6237\n",
            "Epoch [18/20] - Train Loss: 0.5035, Val Loss: 0.6221\n",
            "Epoch [19/20] - Train Loss: 0.4775, Val Loss: 0.6202\n",
            "Epoch [20/20] - Train Loss: 0.4934, Val Loss: 0.6196\n",
            "\n",
            "üîç Testing: LR=0.0001, BS=16, Epochs=30, Optimizer=adam\n",
            "Epoch [1/30] - Train Loss: 0.5800, Val Loss: 0.6273\n",
            "Epoch [2/30] - Train Loss: 0.2008, Val Loss: 0.5765\n",
            "Epoch [3/30] - Train Loss: 0.1112, Val Loss: 0.6394\n",
            "Epoch [4/30] - Train Loss: 0.0355, Val Loss: 0.6945\n",
            "Epoch [5/30] - Train Loss: 0.0182, Val Loss: 0.6979\n",
            "Epoch [6/30] - Train Loss: 0.0423, Val Loss: 0.7022\n",
            "Epoch [7/30] - Train Loss: 0.0082, Val Loss: 0.7132\n",
            "‚èπ Early stopping triggered.\n",
            "\n",
            "üîç Testing: LR=0.0001, BS=16, Epochs=30, Optimizer=sgd\n",
            "Epoch [1/30] - Train Loss: 0.6415, Val Loss: 0.6507\n",
            "Epoch [2/30] - Train Loss: 0.6309, Val Loss: 0.6223\n",
            "Epoch [3/30] - Train Loss: 0.6228, Val Loss: 0.6083\n",
            "Epoch [4/30] - Train Loss: 0.5999, Val Loss: 0.5998\n",
            "Epoch [5/30] - Train Loss: 0.6338, Val Loss: 0.5954\n",
            "Epoch [6/30] - Train Loss: 0.5808, Val Loss: 0.5903\n",
            "Epoch [7/30] - Train Loss: 0.5541, Val Loss: 0.5872\n",
            "Epoch [8/30] - Train Loss: 0.5672, Val Loss: 0.5861\n",
            "Epoch [9/30] - Train Loss: 0.5688, Val Loss: 0.5848\n",
            "Epoch [10/30] - Train Loss: 0.5841, Val Loss: 0.5842\n",
            "Epoch [11/30] - Train Loss: 0.5346, Val Loss: 0.5810\n",
            "Epoch [12/30] - Train Loss: 0.5261, Val Loss: 0.5805\n",
            "Epoch [13/30] - Train Loss: 0.5283, Val Loss: 0.5782\n",
            "Epoch [14/30] - Train Loss: 0.5335, Val Loss: 0.5768\n",
            "Epoch [15/30] - Train Loss: 0.5184, Val Loss: 0.5749\n",
            "Epoch [16/30] - Train Loss: 0.5157, Val Loss: 0.5739\n",
            "Epoch [17/30] - Train Loss: 0.5251, Val Loss: 0.5710\n",
            "Epoch [18/30] - Train Loss: 0.5047, Val Loss: 0.5689\n",
            "Epoch [19/30] - Train Loss: 0.5151, Val Loss: 0.5667\n",
            "Epoch [20/30] - Train Loss: 0.5096, Val Loss: 0.5650\n",
            "Epoch [21/30] - Train Loss: 0.4716, Val Loss: 0.5637\n",
            "Epoch [22/30] - Train Loss: 0.4725, Val Loss: 0.5638\n",
            "Epoch [23/30] - Train Loss: 0.4633, Val Loss: 0.5610\n",
            "Epoch [24/30] - Train Loss: 0.4699, Val Loss: 0.5605\n",
            "Epoch [25/30] - Train Loss: 0.4374, Val Loss: 0.5587\n",
            "Epoch [26/30] - Train Loss: 0.4595, Val Loss: 0.5589\n",
            "Epoch [27/30] - Train Loss: 0.4303, Val Loss: 0.5551\n",
            "Epoch [28/30] - Train Loss: 0.4310, Val Loss: 0.5547\n",
            "Epoch [29/30] - Train Loss: 0.4252, Val Loss: 0.5526\n",
            "Epoch [30/30] - Train Loss: 0.4077, Val Loss: 0.5505\n",
            "\n",
            "üî• Best Hyperparameters: LR=0.0001, BS=8, Epochs=20, Optimizer=adam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train Final Model with Best Hyperparameters***"
      ],
      "metadata": {
        "id": "R-eOdC-djXck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Best Hyperparameters\n",
        "BEST_LR, BEST_BS, BEST_EPOCHS, BEST_OPTIMIZER = best_model_params\n",
        "\n",
        "# Update DataLoader with Best Batch Size\n",
        "train_loader = DataLoader(train_dataset, batch_size=BEST_BS, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BEST_BS, shuffle=False)\n",
        "\n",
        "# Load Pre-trained ResNet-50 Model\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 1)  # Binary classification\n",
        "model = model.to(device)\n",
        "\n",
        "# Define Loss & Optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "if BEST_OPTIMIZER == \"adam\":\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=BEST_LR)\n",
        "else:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=BEST_LR, momentum=0.9)\n",
        "\n",
        "# Train Model with Best Parameters\n",
        "best_model_loss = float(\"inf\")\n",
        "for epoch in range(BEST_EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images).squeeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n",
        "            outputs = model(images).squeeze(1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{BEST_EPOCHS}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save Best Model\n",
        "    if avg_val_loss < best_model_loss:\n",
        "        best_model_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), \"best_tuned_model.pth\")\n",
        "        print(\"‚úÖ Best model saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0oqOwVjjWxp",
        "outputId": "70f15b12-4d24-4e55-92bf-ab02b703cb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Train Loss: 0.6454, Val Loss: 0.6070\n",
            "‚úÖ Best model saved!\n",
            "Epoch [2/20] - Train Loss: 0.2530, Val Loss: 0.6654\n",
            "Epoch [3/20] - Train Loss: 0.1233, Val Loss: 0.6281\n",
            "Epoch [4/20] - Train Loss: 0.0397, Val Loss: 0.6414\n",
            "Epoch [5/20] - Train Loss: 0.0664, Val Loss: 0.6826\n",
            "Epoch [6/20] - Train Loss: 0.0452, Val Loss: 0.6404\n",
            "Epoch [7/20] - Train Loss: 0.1020, Val Loss: 0.7445\n",
            "Epoch [8/20] - Train Loss: 0.1899, Val Loss: 0.7852\n",
            "Epoch [9/20] - Train Loss: 0.0535, Val Loss: 0.7457\n",
            "Epoch [10/20] - Train Loss: 0.0420, Val Loss: 0.8006\n",
            "Epoch [11/20] - Train Loss: 0.0557, Val Loss: 0.9099\n",
            "Epoch [12/20] - Train Loss: 0.0072, Val Loss: 0.9178\n",
            "Epoch [13/20] - Train Loss: 0.0646, Val Loss: 0.9653\n",
            "Epoch [14/20] - Train Loss: 0.0331, Val Loss: 0.9886\n",
            "Epoch [15/20] - Train Loss: 0.0754, Val Loss: 0.9184\n",
            "Epoch [16/20] - Train Loss: 0.0241, Val Loss: 1.1250\n",
            "Epoch [17/20] - Train Loss: 0.0170, Val Loss: 1.0800\n",
            "Epoch [18/20] - Train Loss: 0.0773, Val Loss: 1.4711\n",
            "Epoch [19/20] - Train Loss: 0.0514, Val Loss: 1.7017\n",
            "Epoch [20/20] - Train Loss: 0.0782, Val Loss: 1.7130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluate Final Model***"
      ],
      "metadata": {
        "id": "5TquPfvsj3X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Best Model\n",
        "model.load_state_dict(torch.load(\"best_tuned_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "        outputs = model(images).squeeze(1)\n",
        "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # Convert logits to binary values (0 or 1)\n",
        "\n",
        "        y_true.extend(labels)\n",
        "        y_pred.extend(preds)\n",
        "\n",
        "# Compute Metrics\n",
        "print(f\"\\nüîπ Final Test Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_true, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_X_Xxjlj3yu",
        "outputId": "55b268aa-3508-4ef5-ef82-1683ce04587b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-b27e64a25d16>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_tuned_model.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Final Test Results:\n",
            "Accuracy: 0.7917\n",
            "Precision: 1.0000\n",
            "Recall: 0.2857\n",
            "F1 Score: 0.4444\n"
          ]
        }
      ]
    }
  ]
}